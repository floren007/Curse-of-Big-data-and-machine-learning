{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "496631ea",
   "metadata": {},
   "source": [
    "# Módulo 1: Análisis de datos en el ecosistema Python"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3a14cd3f",
   "metadata": {},
   "source": [
    "### Sesión (13)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6acd93e7",
   "metadata": {},
   "source": [
    "**12/12/2022**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fbaae337",
   "metadata": {},
   "source": [
    "## Análisis de calidad del aire\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8346210a",
   "metadata": {},
   "source": [
    "En los últimos años, los **altos niveles de contaminación** durante ciertos periodos secos en **Madrid** ha obligado a las autoridades a tomar medidas contra el uso de automóviles en el centro de la ciudad, y ha sido utilizado como razón para **proponer modificaciones drásticas en el urbanismo de la ciudad**. \n",
    "\n",
    "Gracias a la **web de [Datos Abiertos del Ayuntamiento de Madrid](https://datos.madrid.es/portal/site/egob/menuitem.400a817358ce98c34e937436a8a409a0/?vgnextoid=eba412b9ace9f310VgnVCM100000171f5a0aRCRD&vgnextchannel=eba412b9ace9f310VgnVCM100000171f5a0aRCRD&vgnextfmt=default)**, los datos de calidad del aire están públicamente disponibles e incluyen **datos históricos diarios y horarios de los niveles registrados desde 2001 hasta 2018** y la lista de estaciones que se utilizan para el análisis de contaminación.\n",
    "\n",
    "Vamos a utilizar **una muestra** que se ha preparado en base a estos datos que muestran la **calidad del aire** en varias estaciones de **Madrid** según diferentes variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4a1419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importamos las librerías necesarias \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d67e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modificamos los parámetros de los gráficos en matplotlib\n",
    "from matplotlib.pyplot import rcParams\n",
    "\n",
    "rcParams['figure.figsize'] = 12, 6 # el primer dígito es el ancho y el segundo el alto\n",
    "rcParams[\"font.weight\"] = \"bold\"\n",
    "rcParams[\"font.size\"] = 10\n",
    "rcParams[\"axes.labelweight\"] = \"bold\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "94808235",
   "metadata": {},
   "source": [
    "### Cargar los datos para el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d8161e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el dataset desde un archivo\n",
    "\n",
    "df_aire = pd.read_excel(\"ABT_CALIDAD_AIRE.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437f8cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consultar los registros del DataFrame\n",
    "df_aire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbc151b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conteo de valores perdidos/faltantes  \n",
    "df_aire.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b80d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# La información útil sobre los datos guardados en formato DataFrame\n",
    "df_aire.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f02ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consultar los valores nulos\n",
    "df_aire['Intensidad_Pto_trafico1'].isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b218dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contar los valores nulos para este campo\n",
    "df_aire['Intensidad_Pto_trafico1'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90269f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar los valores nulos para este campo\n",
    "df_aire['Intensidad_Pto_trafico1'][df_aire['Intensidad_Pto_trafico1'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cac395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sacar los registros que contienen valores nulos para este campo\n",
    "df_aire[67801:67901][['Intensidad_Pto_trafico1', 'nombre_estacion', 'anyo','mes', 'dia', 'hora']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d83ba8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consultamos los registros que tienen algún valor nulo\n",
    "df_aire.drop(df_aire.dropna().index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e53af15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# El tamaño esperado para el dataset limpio\n",
    "df_aire.shape[0] - 20367 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ba1ea5f5",
   "metadata": {},
   "source": [
    "Para evitar problemas posteriores, usamos el método ``dropna()`` para limpiar el tablón de valores perdidos (**missing values**) y reiniciamos el índice. Comprobamos la cantidad de las filas filtradas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d850dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_air_filt = df_aire.dropna().reset_index(drop=True)\n",
    "df_air_filt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a3f97e57",
   "metadata": {},
   "source": [
    "### Análisis Exploratorio Inicial, Tratamiento y Limpieza de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52323eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Echamos un vistazo a las características de cada columna\n",
    "df_air_filt.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee327cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consultamos el tipo de datos\n",
    "df_air_filt.dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "add8a1fb",
   "metadata": {},
   "source": [
    "Los algoritmos **entienden de números y no otra cosa!**, con lo cual para que el algoritmo pueda trabajar con toda la información del dataset, los datos se tienen que **transformar en valores numéricos**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73ed764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consultar las variables que son del tipo \"string\"\n",
    "df_air_filt.dtypes[df_air_filt.dtypes=='object']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7a9aa5bc",
   "metadata": {},
   "source": [
    "Salvo la variable `wind_dir` que indica un parámetro meteorológico y lo tendrémos que tratar como una variable categórica más adelante, el resto se pueden quitar por no ser tan relevante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c060f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analizar las direcciones del tiempo\n",
    "df_air_filt['wind_dir'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "529267bc",
   "metadata": {},
   "source": [
    "Se observa que en el conjunto de datos existen datos **no tan descriptivos** como el **nombre de la estación** y alguna información asociada a estas estaciones como puede ser el **año** o el **tipo de estación**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f21100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datos informativos\n",
    "df_air_filt['id_pto_calidad'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389f21b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datos informativos\n",
    "df_air_filt['Pto_trafico4'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b008ba70",
   "metadata": {},
   "source": [
    "En la lista ``columnas_drop`` definimos las columnas a eliminar del dataset por ser meramente informativas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0975afe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "columnas_drop= [\"Pto_trafico1\",\n",
    "                \"Pto_trafico2\",\n",
    "                \"Pto_trafico3\",\n",
    "                \"Pto_trafico4\",\n",
    "                \"Pto_trafico5\",\n",
    "                \"Pto_trafico5\",\n",
    "                \"anyo\",\n",
    "                \"id_pto_calidad\",\n",
    "                \"nombre_estacion\",\n",
    "                \"tipo_estacion_id\",\n",
    "                \"fecha\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "54217523",
   "metadata": {},
   "source": [
    "Creamos un nuevo dataset que sea como el tablón anterior, eliminando las columnas de la lista indicada. Utilizamos el método `drop` para _DataFrames_ de _pandas_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35a3b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_air_filt2 = df_air_filt.drop(columns=columnas_drop)\n",
    "print(\"Tamaño del tablón filtrado: \", df_air_filt.shape)\n",
    "print(\"Tamaño del tablón nuevo: \", df_air_filt2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a381dee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tipología de las variables exsitentes en el nuevo dataset\n",
    "df_air_filt2.dtypes.value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dd791df1",
   "metadata": {},
   "source": [
    "Vamos a analizar el caso de la única variable todavía no-numérica (`wind_dir`) que necesita un tratamiento concreto. _sklearn_ posee directamente métodos para convertir estas variables en numéricas. Así, tenemos:  \n",
    "\n",
    "* ``sklearn.preprocessing.LabelEncoder``: Recibe un array de strings o enteros y nos devuelve uno de enteros con valores comprendidos **entre _0_ y _n-1_**, donde **_n_ es el número total de categorías** de la variable.  \n",
    "\n",
    "* El problema de _LabelEncoder_ es que para más de dos clases, el algoritmo podrá entender que seguimos teniendo una relación de orden entre los datos, es decir: Una variable categórica que por ejemplo recoja puntos cardinales (N,S,E,O) indica diferentes valores pero el norte no es de mayor ni menor importancia que el oeste. Simplemente, es distinto. Si aplicamos el método _LabelEncoder_ nos devolverá un array con valores entre (0,1,2,3). En algunos casos y **algunos algoritmos pueden deducir que en estos datos existe una relación de orden**, lo cual no es cierto.  \n",
    "\n",
    "* Para evitar este problema, se recurre a ``sklearn.preprocessing.OneHotEncoder``. Este **genera _n-1_ variables \"dummies\" o binarias**, es decir, que toman valores (0,1). Aquí ya se evita el que pueda inferirse un orden en las categorías, pues aquí sí que la variable significa _1== \"es norte\"_ y _0 ==\"no es norte\"_ y así sucesivamente.  \n",
    "El problema que se presenta en este caso, es que **si tenemos muchas variables categóricas con muchas tipologías y clases, al convertirlas en binarias, se nos aumenta notablemente el tamaño del dataset**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5aa87a12",
   "metadata": {},
   "source": [
    "En nuestro dataset hemos visto que hay una variable llamada **\"wind_dir\"** que indica la dirección del viento. Esta variable tiene 16 categorías:  \n",
    "- 'NE'  \n",
    "- 'ENE'  \n",
    "- 'E'\n",
    "- 'ESE'\n",
    "- 'SE'\n",
    "- 'SSE'\n",
    "- 'S'\n",
    "- 'SSW'\n",
    "- 'SW'\n",
    "- 'WSW'\n",
    "- 'W'\n",
    "- 'NNE'\n",
    "- 'N'\n",
    "- 'WNW'\n",
    "- 'NW'\n",
    "- 'NNW'\n",
    "\n",
    "Primero vamos a **agruparlas en cuatro grupos de (N,S,E,O)** y después convertirlas en valres numéricos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db16c5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# La variable de dirección de viento que se requiere agrupar y posteriormente convertirse en números\n",
    "df_air_filt2['wind_dir'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035a51a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilizamos estas listas como categorías para poder agruparlos posteriormente\n",
    "norte = ['NNE','NNW','NE','N']\n",
    "sur = ['SSE','SSW','SW','S']\n",
    "este = ['ENE','ESE','SE','E']\n",
    "oeste = ['WNW','WSW','NW','W']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b6a8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos una nueva columna en una nueva DataFrame con los datos agrupados\n",
    "df_air_filt3 = df_air_filt2.copy()\n",
    "df_air_filt3['wind_dir'] = df_air_filt3['wind_dir'].apply(lambda x: \"N\" if x in norte else\n",
    "                                                                    \"S\" if x in sur else\n",
    "                                                                    \"O\" if x in oeste else \"E\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e75603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consultamos los nuevos valores reemplazados\n",
    "df_air_filt3['wind_dir'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6c6203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprobamos que están todas las celdas bien agrupadas\n",
    "df_air_filt3.groupby('wind_dir').count()['mes']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2fc9dfd2",
   "metadata": {},
   "source": [
    "Una vez que tenemos ya la columna actualizada, vamos a utilizar los __encoders__ de _sklearn_ para transformarlos. Para no añadir más columnas al dataset procedemos con el método _LabelEncoder_:\n",
    "* La función ``sklearn.preprocessing.LabelEncoder`` codifica las etiquetas de una variable categórica en valores numéricos **entre 0 y el número de clases menos 1**.  \n",
    "\n",
    "* Una vez instanciado el encoder, el método ``fit`` lo entrena, **creando el mapeado entre las etiquetas y los números** según las distintas categorías presentes en dicha variable.   \n",
    "\n",
    "* El método ``transform`` asigna para cada etiqueta los números correspondientes, **aplicando el mapeado** creado en el paso anterior.  \n",
    "\n",
    "* El método ``fit_transform`` realiza **ambas acciones conjuntamente**.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4410ef3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "etiquetado = LabelEncoder()\n",
    "etiquetado.fit(df_air_filt3['wind_dir'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4f4caf42",
   "metadata": {},
   "source": [
    "Un atributo de esta función llamado ``classes_`` almacena **el array que mapea las etiquetas** y asigna los números según el índice de cada etiqueta en el array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e78be24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenemos las propiedades:\n",
    "etiquetado.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658eef3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformamos el dataset aplicando el mapeado:\n",
    "etiquetado.transform(df_air_filt3['wind_dir'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "270fa29c",
   "metadata": {},
   "source": [
    "Se ve que al aplicar el mapeado con el método _transform_ obtenemos un array de numpy de tipo enteros. Vamos a añadir en **una nueva columna** llamada **'Dir_viento_etiquetado'** con estos valores númericos y consultamos si existe una relación entre la columna `wind_dir` y esta nueva columna de `Dir_viento_etiquetado` antes de borrar los datos no-numéricos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b04bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asignar una nueva columna\n",
    "df_air_filt3['Dir_viento_etiquetado'] = etiquetado.transform(df_air_filt3['wind_dir'])\n",
    "\n",
    "# mostrar la relación con el dato original\n",
    "df_air_filt3.groupby(['wind_dir','Dir_viento_etiquetado']).count()[['mes']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c4a7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos un nuevo dataframe con solamente datos transformados a valores numéricos\n",
    "df_air_filt4 = df_air_filt3.drop('wind_dir', axis='columns')\n",
    "\n",
    "# Consultar el tipo de las variables\n",
    "df_air_filt4.dtypes.value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1d1cb6ea",
   "metadata": {},
   "source": [
    "### **Reducción de Variables (_Dimensionality Reduction_)**\n",
    "\n",
    "Existen muchos métodos de reducción de variables existentes en _sklearn_.\n",
    "\n",
    "- #### **Filtro por varianza**: \n",
    "\n",
    "Se define un umbral de varianza usando ``from sklearn.feature_selection import VarianceThreshold`` y todas las variables que no lo cumplan se eliminan. **Muy útil para eliminar variables que son casi constantes**. Dispone de los métodos`` fit`` y ``transform`` para aplicar a un dataset. \n",
    "\n",
    "El problema es que devuelve un array con las variables no eliminadas y puede que no resulte sencillo rastrear cuáles ha eliminado. Normalmente hay que ir comparando columna a columna en el dataframe original y el array para ver cuáles ha eliminado y cuáles no.\n",
    "\n",
    "\n",
    "- #### **Filtros univariantes basados en una clasificación de p-values**.\n",
    "\n",
    "Según el modelo sea de clasificación o de regresión, se aplica un **test estadístico (_chi cuadrado_, _anova_ respectivametne)** y tras indicar con cuántas variables queremos quedarnos, se crea una clasificación de variables y el modelo selecciona las _k_ con los _p-valores_ menores con mayor grado de independencia entre variables.\n",
    "\n",
    "\n",
    "- #### **Selección basada en árbol de decisión (_Decision Trees Importances_)**.\n",
    "\n",
    "Consiste en **entrenar un árbol de decisión muy sobreajustado** sobre todo el dataset y después quedarse con las variables que expliquen un valor determinado de la información: 90%, 95 %....\n",
    "\n",
    "Este método utiliza un modelo y como veremos más adelante, todos los modelos de sklearn tienen los siguientes métodos:  \n",
    "  - ``.fit(X=conjunto de train de variables independientes, y=variable objetivo del conjunto de train)``  \n",
    "  - ``.predict(X=conjunto de variables independientes)``. Siempre tiene que tener las mismas variables que el que se utilizó para el .fit()  \n",
    "  - ``.score(y_real, y_predicción)`` Devuelve, **para el caso de regresión el $R^2$** del modelo y **para clasificación el accuracy** entendida como el porcentaje de aciertos sobre el total.\n",
    "\n",
    "- #### **Selección basada en métodos recursivos**.\n",
    "Este caso funciona de modo similar a como lo hacen las regresiones \"_backward_\", es decir, se comienza probando todas las variables para ir sacando variables una a una. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "40bc4c3c",
   "metadata": {},
   "source": [
    "### Importancia de variables"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1e9578b8",
   "metadata": {},
   "source": [
    "Definimos el conjunto de las variables de entrada (_variables independientes_) y la variable objetivo (`Calidad_NO2`), y almacenamos esta última en una variable llamada `target`.\n",
    "\n",
    "Importamos desde la librería _sklearn_ la clase para el _árbol de regresión_. Y procedemos a entrenar uno con todo el dataset y así obtener las variables más importantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5eaeca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar la lista de features y la variable target\n",
    "target = 'Calidad_NO2'\n",
    "features = [x for x in df_air_filt4.columns if x!=target]\n",
    "\n",
    "print(target)\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b95855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consultar la variable de salida\n",
    "df_air_filt4[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbd6631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar el algoritmo de árboles de decisión\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Asignar el algortimo e indicar la profundidad máxima del árbol (con un número rotandamente grande para sobreajustar)\n",
    "arbol_importancia = DecisionTreeRegressor(max_depth=len(features)+10, random_state=100)\n",
    "\n",
    "# Entrenar un árbol con todo el conjunto de datos\n",
    "arbol_importancia.fit(X=df_air_filt4[features], y=df_air_filt4[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17679b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprobamos que se obtiene un R^2 muy alto. Lo desesable es que sea 1.\n",
    "y_pred_arbol = arbol_importancia.predict(X=df_air_filt4[features])\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Métricas para evaluar la calidad del modelo\n",
    "print('Mean Absolute Error:', mean_absolute_error(df_air_filt4[target], y_pred_arbol))\n",
    "print('Mean Absolute Percentage Error:', mean_absolute_percentage_error(df_air_filt4[target], y_pred_arbol)*100)\n",
    "print('Mean Squared Error:', mean_squared_error(df_air_filt4[target], y_pred_arbol))\n",
    "print('Root Mean Squared Error:', np.sqrt(mean_squared_error(df_air_filt4[target], y_pred_arbol)))\n",
    "print('R^2 coefficient of determination:', r2_score(df_air_filt4[target], y_pred_arbol))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e3d30c75",
   "metadata": {},
   "source": [
    "El modelo consigue un rendimiento perfecto!! Ahora vamos a sacar **las variables más importantes** que ha detectado el modelo. El **árbol devuelve la importancia de cada una de ellas** en el orden en el que están las columnas en el dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f9d1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importancias de cada variable en el árbol ajustado (Gini importance)\n",
    "arbol_importancia.feature_importances_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ca0491a4",
   "metadata": {},
   "source": [
    "Se observa que el método ``feature_importances_`` devuelve un array con la importancia en tanto por uno. Para asociarlos, podemos crear una serie con los índices de las variables y al lado la importancia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f5119a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un DataFrame con los datos de importancia\n",
    "importancia = pd.DataFrame(arbol_importancia.feature_importances_, index=features, columns=[\"Importancia\"])\n",
    "\n",
    "# Ordenamos los datos\n",
    "importancia.sort_values(by=importancia.columns[0], ascending=False, inplace=True)\n",
    "importancia"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ad3bd785",
   "metadata": {},
   "source": [
    "Podemos obtener una lista con **las variables que aglutinan el 85% de la información**. Procedemos a añadir al DataFrame \"_importancia_\" que hemos creado en el caso anterior **la columna `imp_acum` que acumule la suma**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3784e184",
   "metadata": {},
   "outputs": [],
   "source": [
    "importancia[\"imp_acum\"] = importancia[\"Importancia\"].cumsum()\n",
    "importancia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac983ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conjunto de variables más importantes\n",
    "importancia.loc[importancia['imp_acum']<=0.85]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b64d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos la lista de variables no tan importantes, cortando por el porcentaje de 85% de la información necesaria\n",
    "variables = importancia.loc[importancia['imp_acum']>0.85].index.to_list()\n",
    "print(variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa946c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Variables iniciales: \", len(features))\n",
    "print(\"Variables no importantes (a eliminar): \", len(variables))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "650ca686",
   "metadata": {},
   "source": [
    "Ahora podemos filtrar el tablón para quedarnos solamente con las variables más importantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515ece43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_air_filt5 = df_air_filt4.drop(labels=variables, axis='columns')\n",
    "df_air_filt5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "41496cfd",
   "metadata": {},
   "source": [
    "### **Planteamiento del ejercicio de clasificación**\n",
    "\n",
    "Vamos a crear una variable objetivo de nombre ***Escenario*** con 2 clases como 0,1 del modo que:  \n",
    "\n",
    "* El **nivel 0** se corresponda a los valores **por debajo del percentil 33** de la variable _target_ (`Calidad_NO2`).    \n",
    "* El **nivel 1** se corresponde a los valores **por encima del percentil 33**.  \n",
    "\n",
    "Eliminamos después la columna `Calidad_NO2` del nuevo dataset y procedemos a dividir el datset en conjuntos de train y test (usualmente con un reparto de 80% - 20%).  \n",
    "\n",
    "**Vamos a intentar predecir si la calidad de aire de las distintas zonas está en el nivel 0, 1.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d12cc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacemos una copia del tablón filtrado para no trabajar sobre original\n",
    "df_aire_calidad = df_air_filt5.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a28c7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear la columna nueva \"Escenario\" y borrar la columna target antigua 'Calidad_NO2'\n",
    "df_aire_calidad[\"Escenario\"] = np.where(df_aire_calidad['Calidad_NO2']<df_aire_calidad['Calidad_NO2'].quantile(0.33),0,1)\n",
    "\n",
    "df_aire_calidad.drop(['Calidad_NO2'], axis='columns', inplace=True)\n",
    "df_aire_calidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff124b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aire_calidad.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e954f711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficar la distribución de los valores originales\n",
    "plt.figure(figsize=(20,6))\n",
    "sns.violinplot(data=df_aire_calidad, orient='v')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7969a7",
   "metadata": {},
   "source": [
    "### **Paso 1.**  Obtención y preparación de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39835834",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preparar el conjunto de datos del modelo\n",
    "\n",
    "# Variables independientes (features)\n",
    "X = df_aire_calidad.drop('Escenario', axis='columns')\n",
    "\n",
    "# Variable dependiente (target) que son los niveles de aire con menos contaminación\n",
    "y = df_aire_calidad['Escenario']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0dfba075",
   "metadata": {},
   "source": [
    "Se puede observar la necesidad de llevar todas las variables de entrada a una escala estándar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa753c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importar los objetos necesarios de la librería sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# declarar el tipo de escalamiento y aplicarlo al conjunto de datos\n",
    "escalado = StandardScaler().fit(X)\n",
    "dataset_normal = escalado.transform(X)\n",
    "dataset_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c0fcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lo convertimos en un DataFrame, añadiendole sus etiquetas\n",
    "X_normal = pd.DataFrame(dataset_normal, columns=X.columns)\n",
    "print(type(X_normal))\n",
    "X_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33570831",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_normal.describe().round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c6950b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficar la distribución de los valores estandarizados\n",
    "plt.figure(figsize=(20,6))\n",
    "sns.boxplot(data=X_normal,  orient='v')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e796485d",
   "metadata": {},
   "source": [
    "### **Paso 2.**  Dividir el dataset en Training y Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478e77b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separar los conjuntos de datos de entrenamiento (Training) y de prueba (Test) para las variables de entrada y salida\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_normal, y, test_size=0.2, random_state=88)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15c572c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"test_size\" representa la proporción del conjunto de datos a incluir en la división de Test\n",
    "print(X_train.shape[0])\n",
    "print(X_test.shape[0])\n",
    "X_train.shape[0] + X_test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d77e581",
   "metadata": {},
   "source": [
    "### **Paso 3.** Cargar y elegir el modelo de regresión logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017e9552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar el módulo que corresponde al algoritmo\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Asignar el algoritmo que vamos a aplicar \n",
    "log_r = LogisticRegression(max_iter=1000,\n",
    "                           random_state=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77926f0",
   "metadata": {},
   "source": [
    "### **Paso 4.** Entrenar el modelo de regresión logística con los datos de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13ab632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar el modelo\n",
    "log_r.fit(X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b13e3d75",
   "metadata": {},
   "source": [
    "Ahora que el modelo está entrenado, sacamos las predicciones, analizamos los resultados y obtenemos algunas métricas del modelo basadas en el conjunto de datos de prueba. Según las métricas, podremos observar si el modelo clasificó correctamente todas los niveles definidos de la calidad del aire."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0627fc5f",
   "metadata": {},
   "source": [
    "### **Paso 5.** Obtener las predicciones "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07473f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular las predicciones con el conjunto de prueba\n",
    "y_pred = log_r.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c836bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimir la salida del modelo (los niveles de calidad del aire)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0502286",
   "metadata": {},
   "source": [
    "### **Paso 6.** Evaluación del modelo a través de sus métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0642e036",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46049d1b",
   "metadata": {},
   "source": [
    "Existen otra serie de metricas para calificar los modelos de clasificación que se detallan a continuación. Algunas de estas medidas se resumen en un informe llamado **classification_report**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fd7a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784cf056",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_test, y_pred),\n",
    "                               display_labels=log_r.classes_)\n",
    "disp.plot()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7343b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular el área bajo la curva de funcionamiento del receptor\n",
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274db5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficar la curva ROC\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "\n",
    "RocCurveDisplay.from_predictions(y_test, y_pred)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a3775c7c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c173b605",
   "metadata": {},
   "source": [
    "### **`Ejercicio 13`**\n",
    "\n",
    "Vamos a realizar un **planteamiento ligeramente distinto** para el mismo ejercicio desarrollado en esta sesión. Posteriormente analizamos el rendimiento del nuevo clasificador (todavía del tipo *logit*):\n",
    "\n",
    "**`13.1`** __Features__: Define una nueva lista de variables no muy importantes y filtra las variables originales, tratadas y almacenadas en un paso anterior como __`df_air_filt4`__, ésta vez para quedarnos con el conjunto de características que componen el `95%` de la información necesaria para modelizar y estimar la variable objetivo.\n",
    "\n",
    "**`13.2`** __Target__: Crea mediante un nuevo planteamiento una variable de salida que clasifique solamente la calidad del aire por debajo del primer cuartil (__Q1__).\n",
    "\n",
    "**`13.3`** __Scaler__: Aplica un escalamiento diferente para llevar ésta vez a todos los datos **a una escala entre 0 y 1**. \n",
    "\n",
    "**`13.4`** Crea un nuevo detector usando el método de _regresión logística_ con el nuevo tratamiento y estos nuevos conjuntos de variables de entrada y de salida. Consulta todas las metricas y visualiza las gráficas que muestran el rendimiento del modelo resultante y explica si se puede elegir a este como un buen clasificador de calidad del aire de Madrid en comparación con el anterior modelo desarrollado en la sesión!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7edc7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Solución\n",
    "# Ejercicio 13.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5040e2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Solución\n",
    "# Ejercicio 13.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d030c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Solución\n",
    "# Ejercicio 13.3\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
